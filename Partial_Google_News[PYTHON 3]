import csv
from urllib.request import urlopen as ureq
from bs4 import BeautifulSoup as soup
import traceback 
import time 
import random 

user_agent_list = [
   #Chrome
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (Windows NT 5.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.2; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36',
    #Firefox
    'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)',
    'Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 6.2; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.0; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0)',
    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; Trident/7.0; rv:11.0) like Gecko',
    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; WOW64; Trident/6.0)',
    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.1; Trident/6.0)',
    'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)'
]


# Input is a csv file named urls.csv of links to scrape

with open('urls.csv', 'r') as csvfile:
    reader = csv.reader(csvfile, delimiter='|')
    url_list = []
    for url in reader:
        url_list.append(url[0])
    
    if len(url_list) == 0:
        print ('Yay! Your work is done')
    
    flg = 10
    flg_choice = [4,5,6,7,8,9,11,12]
    
    time1_list = [20,21,22,23,24,25,26,27,28,29,30]   
        
    for i in range(1,100):
        try:
            with open('Search_Results.csv', 'r') as csvfile:
                reader = csv.reader(csvfile)
                data_list = []
                for data in reader:
                    data_list.append(data)

            if i%flg == 0:
                b = random.choice(time1_list)
                time.sleep(b)
                flg = flg + random.choice(flg_choice)
            
            time2_list = range(1,30)
            a = random.choice(time2_list)
            time.sleep(a)

            url = random.choice(url_list)
            print(url)

            user_agent = random.choice(user_agent_list)
            headers = {'User-Agent': user_agent} 

            request = urllib.request.Request(url,headers={'User-Agent': user_agent})
            response = urllib.request.urlopen(request)
            html = response.read()
            htm = soup(html, "html.parser")
            
            url_list.remove(url)
                
            containers = htm.findAll("div", {"class":"g"})
            
            with open('Search_Results.csv', 'w', newline='') as csvfile:
                row = csv.writer(csvfile)

                for data in data_list:
                    row.writerow(data)

                for container in containers:
                    try:
                        title = container.div.div.h3.text
                    except:
                        title = 'NONE'
                    try:
                        source_url = container.div.div.h3.a['href']
                    except:
                        source_url = 'NONE'
                    try:
                        source = container.div.div.div.span.text
                    except:
                        source = 'NONE'
                    try:
                        date = container.findAll("span",{"class":"f nsa fwzPFf"})[0].text
                    except:
                        date = 'NONE'
                    try:
                        summary = container.findAll("div", {"class":"st"})[0].text 
                    except:
                        summary = 'NONE'

                    row.writerow(['%s'%title, '%s'%source,'%s'%source_url,'%s'%date, '%s'%summary])    
        except:
            print("Program has stopped")
            traceback.print_exc()
            with open('urls.csv', 'w') as csvfile:
                row = csv.writer(csvfile)
                for url in url_list:
                    row.writerow(['%s'%url])
            break
